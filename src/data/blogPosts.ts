// This file is auto-generated. Do not edit manually.
import { authors } from './authors';
import { BlogPost, Category } from './types';

export const categories: Category[] = [
  {
    "value": "tutorials",
    "label": "Tutorials",
    "description": "Step-by-step guides and technical walkthroughs"
  },
  {
    "value": "news",
    "label": "News",
    "description": "Latest updates and announcements from Valkey"
  },
  {
    "value": "case-studies",
    "label": "Case Studies",
    "description": "Real-world examples and implementation stories"
  }
];

export const blogPostsRaw = [
  {
    "title": "Introducing the Valkey Glide Go Client: Now in Public Preview!",
    "date": "2025-03-04T07:01:01.000Z",
    "excerpt": "Valkey Glide now supports GO. Read to learn more about the new client designed for performance and developer productivity",
    "content": "<p>Valkey-Glide is pleased to announce the public preview release of the GLIDE(General Language Independent Driver for the Enterprise) Go client. This release brings the power and reliability of Valkey to Go developers with an API designed for performance and developer productivity.</p>\n<p>Valkey GLIDE is a multi-language client for Valkey, designed for operational excellence and incorporating best practices refined through years of experience. GLIDE ensures a consistent and unified client experience across applications, regardless of the programming language.</p>\n<p>Currently, GLIDE supports Java, Node.js, and Python. This announcement introduces the Valkey GLIDE support for Go, expanding support to Go developers and providing new connectivity to Valkey servers, including both standalone and cluster deployments.</p>\n<h2>Why You Should Be Excited</h2>\n<p>The Go client extends Valkey GLIDE to the Go community, offering a robust, client that&#39;s built on the battle-tested Rust core. This client library is a thoughtfully designed experience for Go developers who need reliable, high-performance data access.</p>\n<h2>Key Features</h2>\n<h3>Advanced Cluster Topology Management</h3>\n<p>Connect to your Valkey cluster with minimal configuration. The client automatically detects the entire cluster topology and configures connection management based on industry best practices.</p>\n<pre><code class=\"language-go\">config := api.NewGlideClusterClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379})\n\nclient, err := api.NewGlideClusterClient(config)\n</code></pre>\n<p>The Go client provides advanced topology managements features such as:</p>\n<h4>Automatic Topology Discovery</h4>\n<p>GLIDE automatically discovers all cluster nodes from a single seed node, eliminating the need to manually configure every node address. The NodeAddress can be an IP address, hostname, or fully qualified domain name (FQDN).</p>\n<h4>Dynamic Topology Maintenance</h4>\n<p>Cluster topology can change over time as nodes are added, removed, or when slot ownership changes. GLIDE implements several mechanisms to maintain an accurate view of the cluster:</p>\n<ul>\n<li><strong>Proactive Topology Monitoring</strong>: GLIDE performs periodic background checks for cluster topology changes. This approach ensures a comprehensive and up-to-date view of the cluster, improving availability and reducing tail latency.</li>\n<li><strong>Consensus-Based Resolution</strong>: GLIDE queries multiple nodes for their topology view and selects the one with the highest agreement, reducing the risk of stale or incorrect mappings and ensuring a more accurate and up-to-date cluster view, improving the overall availability of the cluster.</li>\n<li><strong>Efficient Resource Management</strong>: GLIDE employs an efficient algorithm to compare node views and dynamically throttles client-management requests to prevent overloading Valkey servers, ensuring a balance between maintaining an up-to-date topology map and optimizing resource utilization.</li>\n</ul>\n<h3>Enhanced Connection Management</h3>\n<p>Connection management in distributed systems presents unique challenges that impact performance, reliability, and resource utilization. The Go client addresses these challenges with reliable solutions:</p>\n<h4>Proactive Reconnection</h4>\n<p>GLIDE implements a background monitoring system for connection states. By detecting disconnections and initiating reconnections preemptively, the client eliminates the reconnection latency typically experienced when a request discovers a broken connection.</p>\n<h4>Connection Storm Prevention</h4>\n<p>When network events occur, connection storms can overwhelm servers with simultaneous reconnection attempts. GLIDE mitigates this risk through backoff algorithm with jitter that distributes reconnection attempts over time, protecting servers from sudden connection surges.</p>\n<p>Robust connection handling with automatic reconnection strategies ensures your application remains resilient even during network instability:</p>\n<pre><code class=\"language-go\">// Configure a custom reconnection strategy with exponential backoff\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithReconnectStrategy(api.NewBackoffStrategy(\n        5, // Initial delay in milliseconds\n        10, // Maximum attempts\n        50 // Maximum delay in milliseconds\n    ))\n</code></pre>\n<h4>Multiplexed Connection</h4>\n<p>Rather than maintaining connection pools, GLIDE establishes a single multiplexed connection per cluster node. This architectural choice:</p>\n<ul>\n<li>Minimizes the total number of TCP connections to servers</li>\n<li>Reduces system call overhead</li>\n<li>Maintains high throughput through efficient connection pipelining</li>\n<li>Decreases server-side connection management burden</li>\n</ul>\n<h3>Built for Performance</h3>\n<p>The Go client is designed from the ground up with performance in mind while still being simple to use.\nThe Go client provides a synchronous API for simplicity and compatibility with existing Go key-value store clients. While each individual command is blocking (following the familiar patterns in the ecosystem), the client is fully thread-safe and designed for concurrent usage:</p>\n<pre><code class=\"language-go\">// Example of concurrent execution using goroutines\nfunc performConcurrentOperations(client *api.GlideClient) {\n    var wg sync.WaitGroup\n    \n    // Launch 10 concurrent operations\n    for i := 0; i &lt; 10; i++ {\n        wg.Add(1)\n        go func(idx int) {\n            defer wg.Done()\n            key := fmt.Sprintf(&quot;key:%d&quot;, idx)\n            value := fmt.Sprintf(&quot;value:%d&quot;, idx)\n            \n            // Each command blocks within its goroutine, but all 10 run concurrently\n            _, err := client.Set(key, value)\n            if err != nil {\n                fmt.Printf(&quot;Error setting %s: %v\\n&quot;, key, err)\n                return\n            }\n            \n            result, err := client.Get(key)\n            if err != nil {\n                fmt.Printf(&quot;Error getting %s: %v\\n&quot;, key, err)\n                return\n            }\n            \n            fmt.Printf(&quot;Result for %s: %s\\n&quot;, key, result)\n        }(i)\n    }\n    \n    wg.Wait()\n}\n</code></pre>\n<p>Under the hood, the client efficiently handles these concurrent requests by:</p>\n<ol>\n<li>Using a single multiplexed connection per node to pipeline concurrent commands, minimizing socket overhead and system resources</li>\n<li>Implementing thread-safe command execution</li>\n<li>Efficiently routing concurrent commands to the appropriate server nodes</li>\n</ol>\n<p>While the current API is synchronous, the implementation is specifically optimized for concurrent usage through Go&#39;s native goroutines. We would love feedback about whether to add async/channel-based APIs in future releases.</p>\n<h2>Getting Started</h2>\n<p>You can add Valkey GLIDE to your project with the following two commands:</p>\n<pre><code class=\"language-bash\">go get github.com/valkey-io/valkey-glide/go\ngo mod tidy\n</code></pre>\n<p>Then, you can get started connecting to a Valkey standalone server, running locally on port 6379, with the following sample applications:</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;fmt&quot;\n    &quot;github.com/valkey-io/valkey-glide/go/api&quot;\n)\n\nfunc main() {\n    // Connect to a standalone Valkey server\n    config := api.NewGlideClientConfiguration().\n        WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379})\n    \n    client, err := api.NewGlideClient(config)\n    if err != nil {\n        fmt.Println(&quot;Error:&quot;, err)\n        return\n    }\n    defer client.Close()\n    \n    // Test the connection\n    result, err := client.Ping()\n    if err != nil {\n        fmt.Println(&quot;Error:&quot;, err)\n        return\n    }\n    fmt.Println(result) // PONG\n    \n    // Store and retrieve a value\n    client.Set(&quot;hello&quot;, &quot;valkey&quot;)\n    value, _ := client.Get(&quot;hello&quot;)\n    fmt.Println(value) // valkey\n}\n</code></pre>\n<h3>Cluster Mode Connection Setup</h3>\n<p>Need to work with a Valkey cluster?</p>\n<p>Just as easy! The Go client automatically discovers your entire cluster topology from a single seed node. The following sample shows how to connect to a Valkey cluster through a node running locally on port 7001:</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;fmt&quot;\n    &quot;github.com/valkey-io/valkey-glide/go/api&quot;\n)\n\nfunc main() {\n    // Specify the address of any single node in your cluster\n    // This example connects to a local cluster node on port 7001\n    host := &quot;localhost&quot;\n    port := 7001\n    \n    // Connect to a Valkey cluster through any node\n    config := api.NewGlideClusterClientConfiguration().\n        WithAddress(&amp;api.NodeAddress{Host: host, Port: port})\n    \n    client, err := api.NewGlideClusterClient(config)\n    if err != nil {\n        fmt.Println(&quot;There was an error: &quot;, err)\n        return\n    }\n    \n    res, err := client.Ping()\n    if err != nil {\n        fmt.Println(&quot;There was an error: &quot;, err)\n        return\n    }\n    fmt.Println(res) // PONG\n    client.Close()\n}\n</code></pre>\n<h2>Advanced Configuration Options</h2>\n<h3>Read Strategies for Optimized Performance</h3>\n<p>Balance consistency and throughput with flexible read strategies:</p>\n<pre><code class=\"language-go\">// Configure to prefer replicas for read operations\nconfig := api.NewGlideClusterClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;cluster.example.com&quot;, Port: 6379}).\n    WithReadFrom(api.PreferReplica)\n\nclient, err := api.NewGlideClusterClient(config)\n\n// Write to primary\nclient.Set(&quot;key1&quot;, &quot;value1&quot;)\n\n// Automatically reads from a replica (round-robin)\nresult, err := client.Get(&quot;key1&quot;)\n</code></pre>\n<p>Available strategies:</p>\n<ul>\n<li><strong>PRIMARY</strong>: Always read from primary nodes for the freshest data</li>\n<li><strong>PREFER_REPLICA</strong>: Distribute reads across replicas in round-robin fashion, falling back to primary when needed</li>\n</ul>\n<p>Planned for future release:</p>\n<ul>\n<li><strong>AZ_AFFINITY</strong>: (Coming soon) Prefer replicas in the same availability zone as the client</li>\n</ul>\n<h3>Authentication and TLS</h3>\n<p>Secure your connections with built-in authentication and TLS support:</p>\n<pre><code class=\"language-go\">// Configure with authentication\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithCredentials(api.NewServerCredentials(&quot;username&quot;, &quot;password&quot;)).\n    WithUseTLS(true) // Enable TLS for encrypted connections\n</code></pre>\n<h3>Request Timeout and Handling</h3>\n<p>Fine-tune timeout settings for different workloads:</p>\n<pre><code class=\"language-go\">// Set a longer timeout for operations that may take more time\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithRequestTimeout(500) // 500ms timeout\n</code></pre>\n<h2>Behind the Scenes: Technical Architecture</h2>\n<p>The Valkey GLIDE Go client is built on top of the Valkey GLIDE core. The core framework is written in Rust (lib.rs), which exposes public functions. These functions are converted to a C header file using Cbindgen. The Go client then uses CGO to call these C functions, providing Go developers with an idiomatic interface while leveraging Rust&#39;s performance advantages. This architecture ensures consistent behavior across all Valkey GLIDE language implementations (Java, Python, Node.js, and Go) while maintaining performance and reliability.</p>\n<h3>Component details</h3>\n<pre><code class=\"language-text\">+------------+      +------+      +------------+      +------------+      +------------+\n|            |      |      |      |            |      |            |      |            |\n|    Go      |-----&gt;|      |-----&gt;|  C Header  |-----&gt;|    Rust    |-----&gt;|   Valkey   |\n|  Client    |      |  CGO |      |  cbindgen  |      |    Core    |      |   Server   |\n|            |&lt;-----|      |&lt;-----|            |&lt;-----|            |&lt;-----|            |\n|            |      |      |      |            |      |            |      |            |\n+------------+      +------+      +------------+      +------------+      +------------+\n</code></pre>\n<ul>\n<li><strong>Go Client</strong>: The language-specific interface for Go developers</li>\n<li><strong>CGO</strong>: Allows Go code to call C functions</li>\n<li><strong>Cbindgen</strong>: Automates the generation of C header files from Rust public APIs</li>\n<li><strong>Rust Core</strong>: High-performance framework that connects to and communicates with Valkey servers</li>\n<li><strong>Rust FFI Library</strong>: Enables cross-language function calls between Rust and other languages</li>\n</ul>\n<h2>Join the Journey</h2>\n<p>This public preview is just the beginning. We&#39;re actively developing and enhancing the Go wrapper, and we&#39;d love your feedback and contributions. Try it out in your projects, share your experiences, and help us make it even better!\nYou can join our development journey by:</p>\n<ul>\n<li>Submitting issues or feature requests on our <a href=\"https://github.com/valkey-io/valkey-glide/issues\">GitHub Issues page</a></li>\n<li>Joining discussions in our <a href=\"https://github.com/valkey-io/valkey-glide/discussions\">GitHub Discussions forum</a></li>\n</ul>\n<h2>Looking Forward</h2>\n<p>As we move toward general availability, we&#39;ll be expanding command support, enhancing performance, and adding even more features to make the Valkey GLIDE Go client a great choice for Go developers.</p>\n<p>Checkout our <a href=\"https://github.com/valkey-io/valkey-glide/tree/main/go\">Valkey GLIDE go client</a> for the source code.\nFor implementation examples, please refer to the <a href=\"https://github.com/valkey-io/valkey-glide/blob/main/go/README.md\">README of the Go examples</a> for instructions on running the Standalone and Cluster examples.</p>\n<p>For a complete reference of all available commands and their parameters, explore the <a href=\"https://pkg.go.dev/github.com/valkey-io/valkey-glide/go/api\">Go API documentation on pkg.go.dev</a>, which provides detailed information on method signatures, parameters, and return types.</p>\n<h2>Contributors</h2>\n<p>A huge thank you to all the contributors who have made this possible - your dedication and expertise have created something truly special for the Go community.</p>\n<p><a href=\"https://github.com/janhavigupta007\">Janhavi Gupta</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/niharikabhavaraju\">Niharika Bhavaraju</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/EdricCua\">Edric Cuartero</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/omangesg\">Omkar Mestry</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/Yury-Fridlyand\">Yury Fridlyand</a> (Improving)</p>\n<p><a href=\"https://github.com/prateek-kumar-improving\">Prateek Kumar</a> (Improving)</p>\n<p>Kudos to <a href=\"https://github.com/aaron-congo\">Aaron Congo</a> who created the backbone of the client 🚀 and to <a href=\"https://github.com/umit\">Umit Unal</a>, <a href=\"https://github.com/MikeMwita\">Michael</a> for their contributions!</p>\n",
    "slug": "2025-03-4-go-client-in-public-preview",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "niharikabhavaraju"
    ],
    "trending": false
  },
  {
    "title": "Reducing application latency and lowering Cloud bill by setting up your client library",
    "date": "2025-01-08T07:01:01.000Z",
    "excerpt": "By implementing AZ affinity routing in Valkey and using GLIDE, you can achieve lower latency and cost savings by routing requests to replicas in the same AZ as the client.",
    "content": "<p>How can adjusting your client library help you reduce Cloud costs and improve latency?</p>\n<p>In this post, we dive into <strong>Availability Zone (AZ) affinity routing</strong> mechanics, showing how it optimizes your application&#39;s performance and cost using <strong>Valkey GLIDE (General Language Independent Driver for the Enterprise)</strong>. We also guide you through how to configure GLIDE to benefit from other key features. </p>\n<p>GLIDE is an official open-source Valkey client library. GLIDE is designed for reliability, optimized performance, and high-availability, for Valkey and Redis-based applications. GLIDE is a multi-language client that supports Java, Python, and Node.js, with further languages in development. GLIDE recently added support for a key feature AZ affinity routing, which enables Valkey-based applications to direct calls specifically to server nodes in the same AZ as the client. This minimizes cross-AZ traffic, reduces latency, and lowers cloud expenses.</p>\n<p>Before we explore AZ affinity routing, let’s understand what availability zones are, how different <strong>read strategies</strong> work and how they impact your application.</p>\n<h2>Choosing the Right Read Strategy for Your Application</h2>\n<p>Distributed applications rely on scalability and resilience, often achieved by techniques like caching, sharding, and high availability. Valkey enhances these systems by acting as a robust caching layer, reducing database load and accelerating read operations. Its sharding capabilities distribute data across multiple nodes, ensuring efficient storage and access patterns, while its high availability features safeguard uptime by replicating data across the primary and replica nodes. This combination enables distributed applications to handle high traffic and recover quickly from failures, ensuring consistent performance.</p>\n<p>In Valkey-based applications, selecting the right read strategy is required for optimizing performance and cost. Read strategies determine how read-only commands are routed, balancing factors like data freshness, latency, and throughput. \nUnderstanding the infrastructure that supports these strategies is key to leveraging them effectively.</p>\n<p><strong>Availability Zones</strong> are isolated locations within Cloud regions that provide redundancy and fault tolerance. They are physically separated but connected through low-latency networks. Major cloud providers like AWS, Oracle and GCP implement the concept of AZs. However, using resources across different AZs can incur increased latency and cost.\nGLIDE takes advantage of this infrastructure by routing reads to replica nodes within the same AZ, enabling faster responses and improved user experience. \nThis is particularly advantageous for applications that prioritize read throughput and can tolerate slightly stale data. For instance, websites with personalized recommendation engines rely on displaying content quickly to users rather than ensuring every update is perfectly synchronized.\nAdditionally, one of the most common use cases for caching is to store database query results, allowing applications to trade off absolute freshness for better performance, scalability, and cost-effectiveness. The read-from-replica strategies introduces minimal additional staleness, making it an efficient choice for such scenarios.\nGLIDE provides flexible options tailored to your application’s needs:</p>\n<ul>\n<li><code>PRIMARY</code>: Always read from the primary to ensure the freshness of data.</li>\n<li><code>PREFER_REPLICA</code>: Distribute requests among all replicas in a round-robin manner. If no replica is available, fallback to the primary.</li>\n<li><code>AZ_AFFINITY</code>: Prioritize replicas in the same AZ as the client. If no replicas are available in the zone, fallback to other replicas or the primary if needed.</li>\n</ul>\n<p>In Valkey 8,  <code>availability-zone</code> configuration was introduced, allowing clients to specify the AZ for each Valkey server. GLIDE leverages this new configuration to empower its users with the ability to use AZ Affinity routing. At the time of writing, GLIDE is the only Valkey client library supporting the AZ Affinity strategy, offering a unique advantage.</p>\n<h2>AZ Affinity routing advantages</h2>\n<ol>\n<li><p><strong>Reduce Data Transfer Costs</strong> Cross-zone data transfer often incurs additional charges in Cloud environments. By ensuring operations are directed to nodes within the same AZ, you can minimize or eliminate these costs.</p>\n<p> <strong>Example:</strong> An application in AWS with a Valkey cluster of 2 shards, each with 1 primary and 2 replicas, the instance type is m7g.xlarge. The cluster processes 250MB of data per second and to simplify the example 100% of the traffic is read operation. 50% of this traffic crosses AZs at a cost of $0.01 per GB, the monthly cross-AZ data transfer cost would be approximately $3,285. In addition the cost of the cluster is $0.252 per hour per node. Total of $1,088 per month. By implementing AZ affinity routing, you can reduce the total cost from $4,373 to $1,088, as all traffic remains within the same AZ.</p>\n</li>\n<li><p><strong>Minimize Latency</strong> Distance between AZs within the same region— for example, in AWS, is typically up to 60 miles (100 kilometers)—adds extra roundtrip latency, usually in the range of 500µs to 1000µs. By ensuring requests remain within the same AZ, you can reduce latency and improve the responsiveness of your application.</p>\n<p> <strong>Example:</strong>\n Consider a cluster with three nodes, one primary and two replicas. Each node is located in a different availability zone. The client located in az-2 along with replica-1. </p>\n<p> <strong>With <code>PREFER_REPLICA</code> strategy</strong>:\n In this case, the client will read commands from any replica that is available. It may be located in a different AZ as shown below, and the average latency is, for example, 800 microseconds.</p>\n<p> <img src=\"/src/assets/media/pictures/PREFER_REPLICA_strategy.png\" alt=\"PREFER_REPLICA Read strategy latency example\" /></p>\n<p> <strong>With <code>AZ_AFFINITY</code> strategy</strong>:\n In this case, the client will read commands from a replica in the same client&#39;s AZ and the average latency is, for example, about 300 microseconds.</p>\n<p> <img src=\"/src/assets/media/pictures/AZ_AFFINITY_strategy.png\" alt=\"AZ_AFFINITY Read strategy latency example\" /></p>\n</li>\n</ol>\n<h2>Configuring AZ Affinity Connections with GLIDE</h2>\n<p>Setting up AZ affinity routing in GLIDE is simple, allowing you to leverage its full potential with just a few configuration steps. Let’s walk through the steps to enable this feature in your application.</p>\n<h3>Steps to Set Up AZ Affinity Routing in GLIDE</h3>\n<ol>\n<li><p>Configure Valkey nodes availability zone - \n Assign each Valkey node to a specific AZ based on its physical or virtual location within the Cloud provider&#39;s region. \n The initial configuration must to be done with a separate management client on node initialization, as the clients gets the info from the replicas on the first reconnect. \n In some managed services like Amazon ElastiCache, this mapping is configured automatically and this step is not required. </p>\n<p> For each node, run the following command and change the AZ and routing address as appropriate:</p>\n<p> <strong>Python:</strong></p>\n<pre><code class=\"language-python\">client.config_set({&quot;availability-zone&quot;: az}, \n                    route=ByAddressRoute(host=&quot;address.example.com&quot;, port=6379))\n</code></pre>\n<p> <strong>Java:</strong></p>\n<pre><code class=\"language-Java\">client.configSet(Map.of(&quot;availability-zone&quot;, az), new ByAddressRoute(&quot;address.example.com&quot;, 6379))\n</code></pre>\n<p> <strong>Node.js:</strong></p>\n<pre><code class=\"language-javascript\">client.configSet({&quot;availability-zone&quot;: az}, { route: {type: &quot;routeByAddress&quot;, host:&quot;address.example.com&quot;, port:6379}})\n</code></pre>\n</li>\n<li><p>Configure GLIDE with AZ-Specific Targeting - \n Here are Python, Java, and Node.JS examples showing how to set up an AZ affinity client that directs calls to nodes in the same AZ as the client.</p>\n<p> <strong>Python:</strong></p>\n<pre><code class=\"language-python\">from glide import (\n    GlideClient,\n    GlideClientConfiguration,\n    NodeAddress,\n    ReadFrom\n)\n\n# Determine the client&#39;s AZ (this could be fetched from the cloud provider&#39;s metadata service)\nclient_az = &#39;us-east-1a&#39;\n\n# Initialize Valkey client with preference for the client&#39;s AZ\naddresses = [NodeAddress(host=&quot;address.example.com&quot;, port=6379)]\nclient_config = GlideClusterClientConfiguration(addresses, read_from=ReadFrom.AZ_AFFINITY, client_az=client_az)\nclient = await GlideClusterClient.create(client_config)\n\n# Write operation (route to the primary&#39;s slot owner)\nclient.set(&quot;key1&quot;, &quot;val1&quot;)\n\n# Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nvalue = client.get(&quot;key1&quot;)\n</code></pre>\n<p> <strong>Java:</strong></p>\n<pre><code class=\"language-Java\">// Initialize Valkey client with preference for the client&#39;s AZ\nGlideClusterClientConfiguration config = GlideClusterClientConfiguration.builder()\n    .address(NodeAddress.builder()\n        .host(&quot;address.example.com&quot;)\n        .port(6379)\n        .build())\n    .readFrom(ReadFrom.AZ_AFFINITY)\n    .clientAZ(&quot;us-east-1a&quot;)\n    .build()\nGlideClusterClient client = GlideClusterClient.createClient(config).get();\n\n// Write operation (route to the primary&#39;s slot owner)\nclient.set(&quot;key1&quot;, &quot;val1&quot;).get();\n\n// Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nclient.get(&quot;key1&quot;).get();\n</code></pre>\n<p> <strong>Node.js:</strong></p>\n<pre><code class=\"language-javascript\">import GlideClusterClient from &quot;@valkey/valkey-glide&quot;;\n\nconst addresses = [\n    {\n        host: &quot;address.example.com&quot;,\n        port: 6379\n    }\n];\n\n// Initialize Valkey client with preference for the client&#39;s AZ\nconst client = await GlideClusterClient.createClient({\n    addresses: addresses,\n    readFrom: &quot;AZAffinity&quot; as ReadFrom,\n    clientAz: &quot;us-east-1a&quot;,\n});\n// Write operation (route to the primary&#39;s slot owner)\nawait client.set(&quot;key1&quot;, &quot;val1&quot;);\n// Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nawait client.get(&quot;key1&quot;);\n</code></pre>\n</li>\n</ol>\n<h2>Conclusion</h2>\n<p>By implementing AZ affinity routing in Valkey and using GLIDE, you can achieve lower latency and cost savings by routing requests to replicas in the same AZ as the client.</p>\n<h3>Further Reading</h3>\n<ul>\n<li><a href=\"https://github.com/valkey-io/valkey-glide\">Valkey GLIDE GitHub Repository</a></li>\n<li><a href=\"https://valkey.io/\">Valkey Documentation</a></li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/Python-wrapper#read-strategy\">Valkey GLIDE read strategy documentation in Python</a> </li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/Java-Wrapper#read-strategy\">Valkey GLIDE read strategy documentation in Java</a></li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/NodeJS-wrapper#read-strategy\">Valkey GLIDE read strategy documentation in NodeJS</a></li>\n</ul>\n",
    "slug": "az-affinity-strategy",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "asafporatstoler",
      "adarovadya"
    ],
    "trending": false
  },
  {
    "title": "Generally Available: Valkey 8.0.0",
    "date": "2024-09-16T07:01:01.000Z",
    "excerpt": "Today marks a milestone for the Valkey project: the first major release.",
    "content": "<p>The first ever release of Valkey, 7.2.5, became generally available more than 5 months ago.\nWhile the initial release was a milestone, it focused on compatibility and license continuity; bringing no new features to the table.\nToday marks a different milestone for the Valkey project: the first major release.\nValkey 8.0.0 continues the traditions of the seven major versions of Redis that precede it by bringing improvements to speed and efficiency alongside new features.</p>\n<p>Key properties of the Valkey project are transparency and collaboration.\nAs a consequence of Valkey 8.0.0 being developed entirely in the open, the team has already written about both the big and small features of the release.\nThe best overview is the RC1 blog which breaks down all the changes and features in the release into a few sections: <a href=\"/blog/valkey-8-0-0-rc1/#performance\">performance</a>, <a href=\"/blog/valkey-8-0-0-rc1/#reliability\">reliability</a>, <a href=\"/blog/valkey-8-0-0-rc1/#replication\">replication</a>, <a href=\"https://valkey.io/blog/valkey-8-0-0-rc1/#observability\">observability</a>, and <a href=\"/blog/valkey-8-0-0-rc1/#efficiency\">efficiency</a>.\nAdditionally, there are deep dives on the <a href=\"/blog/unlock-one-million-rps/\">speed</a> (with a <a href=\"/blog/unlock-one-million-rps-part2/\">follow up</a>) and <a href=\"/blog/valkey-memory-efficiency-8-0/\">efficiency</a> improvements in Valkey 8.0.0.</p>\n<p>While this is a major version, Valkey takes command set compatibility seriously: Valkey 8.0.0 makes no backwards incompatible changes to the existing command syntax or their responses.\nYour existing tools and custom software will be able to immediately take advantage of Valkey 8.0.0.\nSince Valkey 8.0.0 does make some small changes to previously undefined behaviors, it&#39;s wise to <a href=\"https://github.com/valkey-io/valkey/blob/8.0.0/00-RELEASENOTES\">read the release notes</a>.\nAdditionally, because this version makes changes in how the software uses threading, you may want to re-evaluate your cluster’s infrastructure to achieve the highest performance.</p>\n<p>Valkey 8.0.0 has gone through multiple rounds of release candidates, testing, and verification.\nThe Technical Steering Committee considers it ready for production usage.\nYou can <a href=\"https://github.com/valkey-io/valkey/tree/8.0.0\">build from source</a>, start <a href=\"/download/\">installing the binaries, or deploy the containers</a> today.\nExpect package managers to pick up the latest version in the coming days.</p>\n<p><strong>Note:</strong> <a href=\"https://github.com/valkey-io/valkey/tree/8.0.1\">Valkey 8.0.1</a> was released on October 2, read the release notes on <a href=\"https://github.com/valkey-io/valkey/blob/8.0.1/00-RELEASENOTES\">GitHub</a>.</p>\n",
    "slug": "valkey-8-ga",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Unlock 1 Million RPS: Experience Triple the Speed with Valkey - part 2",
    "date": "2024-09-13T07:01:01.000Z",
    "excerpt": "Maximize the performance of your hardware with memory access amortization",
    "content": "<p> In the <a href=\"/blog/unlock-one-million-rps/\">first part</a> of this blog, we described how we offloaded almost all I/O operations to I/O threads, thereby freeing more CPU cycles in the main thread to execute commands. When we profiled the execution of the main thread, we found that a considerable amount of time was spent waiting for external memory. This was not entirely surprising, as when accessing random keys, the probability of finding the key in one of the processor caches is relatively low.  Considering that external memory access latency is approximately 50 times higher than L1 cache, it became clear that despite showing 100% CPU utilization, the main process was mostly “waiting”. In this blog, we describe the technique we have been using to increase the number of parallel memory accesses, thereby reducing the impact that external memory latency has on performance.</p>\n<h3>Speculative execution and linked lists</h3>\n<p>Speculative execution is a performance optimization technique used by modern processors, where the processor guesses the outcome of conditional operations and executes in parallel subsequent instructions ahead of time. Dynamic data structures, such as linked lists and search trees, have many advantages over static data structures: they are economical in memory consumption, provide fast insertion and deletion mechanisms, and can be resized efficiently. However, some dynamic data structures have a major drawback: they hinder the processor&#39;s ability to speculate on future memory load instructions that could be executed in parallel. This lack of concurrency is especially problematic in very large dynamic data structures, where most pointer accesses result in high-latency external memory access.</p>\n<p>In this blog, Memory Access Amortization, a method that facilitates speculative execution to improve performance, is introduced along with how it is applied in Valkey. The basic idea behind the method is that by interleaving the execution of operations that access random memory locations, one can achieve significantly better performance than by executing them serially.</p>\n<p>To depict the problem we are trying to solve consider the following <a href=\"/assets/C/list_array.c\">function</a> which gets an array of linked list and returns sum of all values in the lists:</p>\n<pre><code class=\"language-c\">unsigned long sequentialSum(size_t arr_size, list **la) {\n    list *lp;\n    unsigned long  res = 0; \n\n    for (int i = 0; i &lt; arr_size; i++) { \n        lp = la[i]; \n        while (lp) { \n            res += lp-&gt;val;\n            lp = lp-&gt;next;\n        }\n    }\n\n    return res; \n}\n</code></pre>\n<p>Executing this function on an array of 16 lists containing 10 million elements each takes approximately 20.8 seconds on an ARM processor (Graviton 3). Now consider the following alternative implementation which instead of scanning the lists separately,  interleaves the executions of the lists scans: </p>\n<pre><code class=\"language-c\">unsigned long interleavedSum(size_t arr_size, list **la) {\n    list **lthreads = malloc(arr_size * sizeof(list *)); \n    unsigned long res = 0; \n    int n = arr_size; \n\n    for (int i = 0; i &lt; arr_size; i++) {\n        lthreads[i] = la[i]; \n        if (lthreads[i] == NULL) \n            n--; \n    } \n\n    while(n) {\n        for (int i = 0; i &lt; arr_size; i++) { \n            if (lthreads[i] == NULL) \n                continue; \n            res += lthreads[i]-&gt;val;\n            lthreads[i] = lthreads[i]-&gt;next; \n            if (lthreads[i] == NULL) \n                n--;\n        }  \n    }\n\n    free(lthreads);\n    return res; \n}\n</code></pre>\n<p>Running this new version with the same input as previously described takes less than 2 seconds, achieving a 10x speedup! The explanation for this significant improvement lies in the processor&#39;s speculative execution capabilities. In a standard sequential traversal of a linked list, as seen in the first version of the function, the processor cannot &#39;speculate&#39; on future memory access instructions. This limitation becomes particularly costly with large lists, where each pointer access likely results in a expensive external memory access. In contrast, the alternative implementation, which interleaves list traversals, allows the processor to issue more memory accesses in parallel. This leads to an overall reduction in memory access latency through amortization. </p>\n<p>One way to maximize the amount of parallel memory access issued is to add prefetch instructions. Replacing </p>\n<pre><code class=\"language-c\">             if (lthreads[i] == NULL) \n                n--;\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-c\">            if (lthreads[i]) \n                __builtin_prefetch(lthreads[i]);\n            else \n                n--;\n</code></pre>\n<p>reduces the execution time further to 1.8 sec. </p>\n<h3>Back to Valkey</h3>\n<p>In the first part, we described how we updated the existing I/O threads implementation to increase parallelism and reduce the amount of I/O operations executed by the main thread to a minimum. Indeed, we observed an increase in the number of requests per second, reaching up to 780K SET commands per second. Profiling the execution revealed that Valkey&#39;s main thread was spending more than 40% of its time in a single function: lookupKey, whose goal is to locate the command keys in Valkey&#39;s main dictionary. This dictionary is implemented as a straightforward chained hash, as shown in the picture below: \n<img src=\"/src/assets/media/pictures/lookupKey.jpg\" alt=\"dict find\" />\nOn a large enough set of keys, almost every memory address accessed while searching the dictionary will not be found in any of the processor caches, resulting in costly external memory accesses. Also, similarly as with the linked list from above, since the addresses in the table→dictEntry→...dictEntry→robj sequence are serially dependent, it is not possible to determine the next address to be accessed before the previous address in the chain has been resolved.  </p>\n<h3>Batching and interleaving</h3>\n<p>To overcome this inefficiency, we adopted the following approach. Every time a batch of incoming commands from the I/O threads is ready for execution, Valkey’s main thread efficiently prefetches the memory addresses needed for future lookupKey invocations for the keys involved in the commands  before executing the commands. This prefetch phase is achieved by dictPrefetch, which, similarly as with the linked list example from above, interleaves the table→dictEntry→...dictEntry→robj search sequences for all keys. This reduces the time spent on lookupKey by more than 80%. Another issue we had to address was that all the incoming parsed commands from the I/O threads were not present in the L1/L2 caches of the core running Valkey’s main thread. This was also resolved using the same method.  All the relevant code can be found in <a href=\"https://github.com/valkey-io/valkey/blob/unstable/src/memory_prefetch.c\">memory_prefetch.c</a>. In total the impact of the memory access amortization on Valkey performance is almost 50% and it increased the requests per second to more than 1.19M rps. </p>\n<h3>How to reproduce Valkey 8.0 performance numbers</h3>\n<p>This section will walk you through the process of reproducing our performance results, where we achieved 1.19 million requests per second using Valkey 8.</p>\n<h3>Hardware Setup</h3>\n<p>We conducted our tests on an AWS EC2 c7g.4xlarge instance, featuring 16 cores on an ARM-based (aarch64) architecture.</p>\n<h3>System Configuration</h3>\n<blockquote>\n<p>Note: The core assignments used in this guide are examples. Optimal core selection may vary depending on your specific system configuration and workload.</p>\n</blockquote>\n<p>Interrupt affinity - locate the network interface with <code>ifconfig</code> (let&#39;s assume it is <code>eth0</code>) and its associated IRQs with </p>\n<pre><code class=\"language-bash\">grep eth0 /proc/interrupts | awk &#39;{print $1}&#39; | cut -d : -f 1\n</code></pre>\n<p>In our setup, lines <code>48</code> to <code>55</code> are allocated for <code>eth0</code> interrupts. Allocate one core per 4 IRQ lines: </p>\n<pre><code class=\"language-bash\">for i in {48..51}; do echo 1000 &gt; /proc/irq/$i/smp_affinity; done\nfor i in {52..55}; do echo 2000 &gt; /proc/irq/$i/smp_affinity; done\n</code></pre>\n<p>Server configuration - launch the Valkey server with these minimal configurations:</p>\n<pre><code class=\"language-bash\">./valkey-server --io-threads 9 --save --protected-mode no\n</code></pre>\n<p><code>--save</code> disables dumping to RDB file and <code>--protected-mode no </code>  allows connections from external hosts. <code>--io-threads</code> number includes the main thread and the IO threads, meaning that in our case 8 I/O threads are launched in addition to the main thread. </p>\n<p>Main thread affinity - pin the main thread to a specific CPU core, avoiding the cores handling IRQs. Here we use core #3:</p>\n<pre><code class=\"language-bash\">sudo taskset -cp 3 `pidof valkey-server`\n</code></pre>\n<blockquote>\n<p>Important: We suggest experimenting with different core pinning strategies to find the optimal performance while avoiding conflicts with IRQ-handling cores.</p>\n</blockquote>\n<h3>Benchmark Configuration</h3>\n<p>Run the benchmark from a separate instance using the following parameters:</p>\n<ul>\n<li>Value size: 512 bytes</li>\n<li>Number of keys: 3 million</li>\n<li>Number of clients: 650</li>\n<li>Number of threads: 50 (may vary for optimal results)</li>\n</ul>\n<pre><code class=\"language-bash\">./valkey-benchmark -t set -d 512 -r 3000000 -c 650 --threads 50 -h &quot;host-name&quot; -n 100000000000\n</code></pre>\n<blockquote>\n<p>Important: When running the benchmark, it may take a few seconds for the database to get populated and for the performance to stabilize. You can adjust the <code>-n</code> parameter to ensure the benchmark runs long enough to reach optimal throughput.</p>\n</blockquote>\n<h3>Testing and Availability</h3>\n<p><a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc2\">Valkey 8.0 RC2</a> is available now for evaluation with I/O threads and memory access amortization.</p>\n",
    "slug": "unlock-one-million-rps-part2",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "dantouitou",
      "uriyagelnik"
    ],
    "trending": false
  },
  {
    "title": "Storing more with less: Memory Efficiency in Valkey 8",
    "date": "2024-09-04T07:01:01.000Z",
    "excerpt": "Learn about the new memory efficiency improvements in Valkey 8 which reduces memory overhead, which allows more data to be stored in the same amount of memory.",
    "content": "<p>Valkey 8.0 GA is around the corner and one of the themes is increasing overall memory efficiency. Memory overhead reduction has the obvious effect of better resource utilization, but also impacts performance. By minimizing unnecessary memory consumption, you can store more data with the same hardware resources and improve overall system responsiveness. This post is going to give an overview into how Valkey internally manages the data and its memory overhead. Additionally, it talks about the two major improvements for Valkey 8.0 that improves the overall memory efficiency.</p>\n<h2>Overview</h2>\n<p>Valkey has two modes of operation: standalone and cluster mode. Standalone allows for one primary with it’s replica(s). To shard data horizontally and scale to store large amounts of data, cluster mode provides a mechanism to set up multiple primaries each with their own replica(s). </p>\n<p><img src=\"/src/assets/media/pictures/valkey_operation_mode.png\" alt=\"Figure 1 Standalone (left) and Cluster mode (right)\" /></p>\n<p>For both standalone and cluster mode setup, Valkey&#39;s main dictionary is a hash table with a chained linked list: The major components are a <strong>bucket</strong> and <strong>dictionary entry</strong>. A key is hashed to a bucket and each bucket points to a linked list of dictionary entries and further each dictionary entry consists of key, value, and a next pointer. Each pointer takes 8 bytes of memory usage. So, a single dictionary entry has a minimum overhead of 24 bytes.</p>\n<p><img src=\"/src/assets/media/pictures/dictionary_bucket_and_entry_overview.png\" alt=\"Figure 2 Dictionary bucket pointing to a dictionary entry\" /></p>\n<p>In cluster mode, Valkey uses a concept called <a href=\"https://valkey.io/topics/cluster-tutorial/\">hash slots</a> to shard data. There are 16,384 hash slots in cluster, and to compute the hash slot for a given key, the server computes the CRC16 of the key modulo 16,384. Keys are distributed on basis of these slots assigned to each of the primary. The server needs to maintain additional metadata for bookkeeping i.e. slot-to-key mapping to move a slot from one primary to another. In order to maintain the slot to key mapping, two additional pointers <code>slot-prev</code> and <code>slot-next</code> (Figure 3)  are stored as metadata in each dictionary entry forming a double linked list of all keys belonging to a given slot. This further increases the overhead by 16 bytes per dictionary entry i.e. total 40 bytes.</p>\n<p><img src=\"/src/assets/media/pictures/dictionary_in_cluster_mode_7.2.png\" alt=\"Figure 3 Dictionary in cluster mode (Valkey 7.2) with multiple key value pair\" /></p>\n<h2>Improvements</h2>\n<h3>Optimization 1 - <a href=\"https://github.com/redis/redis/pull/11695\">Dictionary per slot</a></h3>\n<p>The first optimization is a dictionary per slot (16,384 of them in total), where each dictionary stores data for a given slot. With this simplification, the cost of maintaining additional metadata for the mapping of slot to key is no longer required in Valkey 8. To iterate over all the keys in a given slot, the engine simply finds out the dictionary for a given slot and traverse all the entries in it. This reduces the memory usage per dictionary entry by 16 bytes with a small memory overhead around 1 MB per node. As cluster mode is generally used for storing large amount of keys, avoiding the additional overhead per key allows users to store more number of keys in the same amount of memory.</p>\n<p><img src=\"/src/assets/media/pictures/dictionary_in_cluster_mode_8.0.png\" alt=\"Figure 4 Dictionary in cluster mode (Valkey 8.0) with multiple key value pair\" /></p>\n<p>A few of the interesting challenges that comes up with the above improvements are supporting existing use cases which were optimized with a single dictionary for the entire keyspace. The usecases are:</p>\n<ul>\n<li><a href=\"https://valkey.io/commands/scan/\">Iterating the entire keyspace</a> - Command like <code>SCAN</code> to iterate over the entire keyspace.</li>\n<li><a href=\"https://valkey.io/topics/lru-cache/\">Random key for eviction</a> - The server does random sampling of the keyspace to find ideal candidate for eviction.</li>\n<li><a href=\"https://valkey.io/commands/randomkey/\">Finding a random key</a> - Commands like <code>RANDOMKEY</code> retrieve a random key from the database.</li>\n</ul>\n<p>In order to efficiently implement these functions, we need to be able to both find non-empty slots, to skip over empty slots during scanning, and be able to select a random slot weighted by the number of keys that it owns. These requirements require a data structure which provides the following functionality:</p>\n<ol>\n<li>Modify value for a given slot - If a key gets added or removed, increment or decrement the value for that given slot by 1 respectively.</li>\n<li>Cumulative frequency until each slot - For a given number representing a key between 1 and the total number of keys, return the slot which covers the particular key.</li>\n</ol>\n<p>If approached naively, the former and latter operation would take O(1) and O(N) respectively. However, we want to minimize the latter operation’s time complexity and minimally avoid in the former. Hence, a <a href=\"https://www.topcoder.com/thrive/articles/Binary%20Indexed%20Trees\">binary indexed tree (BIT) or fenwick tree</a> which provides the above functionality with a minimal memory overhead (~1 MB per node) and the time complexity is also bounded to O(M log N) for both the operations where M = number of modification(s) and N = number of slots. This enables skipping over empty slots efficiently while iterating over the keyspace as well as finding a slot for a given key index in logarithmic time via binary search over the cumulative sum maintained by the BIT. </p>\n<p>Another interesting side effect is on the rehashing operation. Rehashing is CPU intensive. By default, a limited number of buckets are allocated in a dictionary and it expands/shrinks dynamically based on usage. While undergoing rehashing, all the data needs to be moved from an old dictionary to a new dictionary. With Valkey 7.2, a global dictionary being shared across all the slots, all the keys get stored under a single dictionary and each time the fill factor (number of keys / number of buckets) goes above 1, the dictionary needs to move to a larger dictionary (multiple of 2) and move a large amount of keys. As this operation is performed on the fly, it causes an increase in latency for regular command operations while it&#39;s ongoing. With the per-slot dictionary optimization, the impact of rehashing is localized to the specific dictionary undergoing the process and only a subset of keys needs to be moved.</p>\n<p>Overall, with this new approach, the benefits are: </p>\n<ol>\n<li>Removes additional memory overhead in cluster mode: Get rid of two pointers (16 bytes) per key to keep the mapping of slot to keys.</li>\n<li>With the rehashing operation spread out across dictionaries, CPU utilization is also spread out.</li>\n</ol>\n<h3>Optimization 2 - <a href=\"https://github.com/valkey-io/valkey/pull/541\">Key embedding into dictionary entry</a></h3>\n<p>After the dictionary per slot change, the memory layout of dictionary entry in cluster mode is the following, there are three pointers (key, value, and next). The key pointer points to a SDS (<a href=\"https://github.com/antirez/sds/blob/master/README.md\">simple dynamic string</a>) which contains the actual key data. As a key is immutable, without bringing in much complexity, it can be embedded into the dictionary entry which has the same lifetime as the former.</p>\n<p><img src=\"/src/assets/media/pictures/key_embedding.png\" alt=\"Figure 5 Key data storage in 7.2 (left) and 8.0 (right)\" /></p>\n<p>With this new approach, the overall benefits are: </p>\n<ol>\n<li>Reduces 8 bytes additional memory overhead per key.</li>\n<li>Removes an additional memory lookup for key: With access of dictionary entry, the additional random pointer access for key is no longer required leading to better cache locality and overall better performance.</li>\n</ol>\n<h3>Benchmarking</h3>\n<h4>Setup</h4>\n<p>A single shard cluster is setup with 1 primary and 2 replica(s). Each node runs with different version to highlight the memory improvements with each optimization introduced between 7.2 to 8.0 and to signify that no additional configuration is required to achieve the memory efficiency.</p>\n<ul>\n<li>Node A: Primary running on port 6379 with <a href=\"https://github.com/valkey-io/valkey/commit/ad0a24c7421d3a8ea76cf44b56001e3b3b6ed545\">Valkey 7.2 version</a></li>\n<li>Node B: Replica 1 running on port 6380 with <a href=\"https://github.com/valkey-io/valkey/commit/1ea49e5845a11250a13273c725720822c26860f1\">optimization 1 - dictionary per slot</a></li>\n<li>Node C: Replica 2 running on port 6381 with <a href=\"https://github.com/valkey-io/valkey/commit/a323dce8900341328114b86a92078c50cec0d9b8\">optimization 1 - dictionary per slot and optimization 2 - key embedding</a> - Includes all memory efficiency optimization in Valkey 8.</li>\n</ul>\n<h4>Synthetic data generation using <a href=\"https://valkey.io/topics/benchmark/\">valkey-benchmark utility</a></h4>\n<pre><code>src/valkey-benchmark \\\n -t set \\\n -n 10000000 \\\n -r 10000000 \\\n -d 16\n</code></pre>\n<h4>Memory Usage</h4>\n<ul>\n<li>Node A</li>\n</ul>\n<pre><code>127.0.0.1:6379&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6379&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:727339288\nused_memory_human:693.64M\n</code></pre>\n<ul>\n<li>Node B</li>\n</ul>\n<pre><code>127.0.0.1:6380&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6380&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:627851888\nused_memory_human:598.77M\n</code></pre>\n<ul>\n<li>Node C</li>\n</ul>\n<pre><code>127.0.0.1:6381&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6381&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:577300952\nused_memory_human:550.56M\n</code></pre>\n<h4>Overall Improvement</h4>\n<p><img src=\"/src/assets/media/pictures/memory_usage_comparison.png\" alt=\"Figure 6 Overall memory usage with benchmark data\" /></p>\n<h4>With dictionary per slot change memory usage reduced from 693.64 MB to 598.77 MB with the same dataset</h4>\n<ul>\n<li><strong>Percentage Drop 1</strong>: ((693.64 - 598.77) / 693.64) * 100 = (94.87 / 693.64) * 100 ≈ 13.68%</li>\n</ul>\n<h4>Further with key embedding, memory usage reduced from 598.77 MB to 550.56 MB with the same dataset</h4>\n<ul>\n<li><strong>Percentage Drop 2</strong>: ((598.77 - 550.56) / 598.77) * 100 = (48.21 / 598.77) * 100 ≈ 8.05%</li>\n</ul>\n<h4>Overall Drop: From 693.64 MB to 550.56 MB</h4>\n<ul>\n<li><strong>Overall Percentage Drop</strong>: ((693.64 - 550.56) / 693.64) * 100 = (143.08 / 693.64) * 100 ≈ 20.63%</li>\n</ul>\n<p>So, the drop in percentage is approximately <strong>20.63% in overall memory usage on a given node on upgrade from Valkey 7.2 to Valkey 8.0</strong>.</p>\n<h2>Conclusion</h2>\n<p>Through the memory efficiency achieved by introducing dictionary per slot and key embedding into dictionary entry, users should have additional capacity to store more keys per node in Valkey 8.0 (up to 20%, but it will vary based on the workload). For users, upgrading from Valkey 7.2 to Valkey 8.0, the improvement should be observed automatically and no configuration changes are required. \nGive it a try by spinning up a <a href=\"https://valkey.io/download/\">Valkey cluster</a> and join us in the <a href=\"https://github.com/valkey-io/valkey/\">community</a> to provide feedback. Further, there is an ongoing discussion around overhauling the main dictionary with a more compact memory layout and introduce an open addressing scheme which will significantly improve memory efficiency. More details can be found <a href=\"https://github.com/valkey-io/valkey/issues/169\">here</a>.</p>\n",
    "slug": "valkey-memory-efficiency-8-0",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "hpatro"
    ],
    "trending": false
  },
  {
    "title": "Unlock 1 Million RPS: Experience Triple the Speed with Valkey",
    "date": "2024-08-05T07:01:01.000Z",
    "excerpt": "Learn about the new performnace improvements in Valkey 8 which reduces cost, improves latency and makes our environment greener.",
    "content": "<p>Valkey 8.0, set for release in September 2024, will bring major performance enhancements through a variety of improvements including a new multi-threaded architecture.\nThis update aims to significantly boost throughput and reduce latency across various hardware configurations.\nRead on to learn more about the new innovative I/O threading implementation and its impact on performance and efficiency.\nThis post is the first in a two-part series. The next post will dive into the new prefetch mechanism and its impact on performance.</p>\n<h3>Our Commitment to performance and efficiency</h3>\n<p>At AWS, we have hundreds of thousands of customers using Amazon ElastiCache and Amazon MemoryDB.\nFeedback we continuously hear from end users is that they need better absolute performance and want to squeeze more performance from their clusters.</p>\n<p>Our commitment to meeting these performance and efficiency needs led us down a path of improving the multi-threaded performance of our ElastiCache and MemoryDB services, through features we called <a href=\"https://aws.amazon.com/blogs/database/boosting-application-performance-and-reducing-costs-with-amazon-elasticache-for-redis/\">Enhanced IO</a> and <a href=\"https://aws.amazon.com/blogs/database/enhanced-io-multiplexing-for-amazon-elasticache-for-redis/\">Multiplexing</a>.\nToday we are excited to dive into how we are sharing our learnings from this performance journey by contributing a major performance improvement to the Valkey project.</p>\n<h3>Benefits of High Capacity Shards</h3>\n<p>Valkey&#39;s common approach to performance and memory improvement is scaling out by adding more shards to the cluster.\nHowever, the availability of more powerful nodes offers additional flexibility in application design.\nHigher-capacity shards can increase cluster capacity, improve resilience to request surges, and reduce latencies at high percentiles.\nThis approach is particularly beneficial for Valkey users with workloads that don&#39;t respond well to horizontal scaling, such as hot keys and large collections that can&#39;t be effectively distributed across multiple shards.</p>\n<p>Another challenge with horizontal scaling comes from multi-key operations like MGET.\nThese multi-key operations require all involved keys to reside in the same slot, often resulting in users utilizing only a small number of slots, which can significantly restrict the cluster&#39;s scalability potential.\nLarger shards can alleviate these constraints by accommodating more keys and larger collections within a single node.</p>\n<p>While larger shards offer these benefits, they come with trade-offs.\nFull synchronization for very large instances can be risky, and losing a large shard can be more impactful than losing a smaller one.\nConversely, managing a cluster with too many small instances can be operationally complex.\nThe optimal configuration depends on the specific workload, requiring a careful balance between scaling out and using larger shards.</p>\n<h3>Major Upgrade to Valkey Performance</h3>\n<p>Starting with version 8, Valkey users will benefit from an increase in multi-threaded performance, thanks to a new multi-threading architecture that can boost throughput and reduce latency on a wide range of hardware types.\n<img src=\"/src/assets/media/pictures/performance_comparison.png\" alt=\"Performance comparison between existing I/O threading implementation and the new I/O threading implementation available in Valkey 8.\" /></p>\n<p>The data demonstrates a substantial performance improvement with the new I/O threads approach.\nThroughput increased by approximately 230%, rising from 360K to 1.19M requests per second compared to Valkey 7.2\nLatency metrics improved across all percentiles, with average latency decreasing by 69.8% from 1.792 ms to 0.542 ms.</p>\n<p>Tested with 8 I/O threads, 3M keys DB size, 512 bytes value size, and 650 clients running sequential SET commands using AWS EC2 C7g.16xlarge instance.\nPlease note that these numbers include the Prefetch change that will be described in the next <a href=\"/blog/unlock-one-million-rps-part2/\">blog post</a></p>\n<h3>Performance Without Compromising Simplicity</h3>\n<p>Valkey strives to stay simple by executing as much code in a single thread as possible.\nThis ensures an API that can continuously evolve without the need to use complex synchronization and avoid race conditions.\nOur new multi-threading approach is designed based on this long-standing architectural principle that we believe is the right architecture for Valkey.\nIt utilizes a minimal number of synchronization mechanisms and keeps Valkey command execution single-threaded, simple, and primed for future enhancements.</p>\n<p><img src=\"/src/assets/media/pictures/io_threads.png\" alt=\"I/O threads high level design\" /></p>\n<h3>High Level Design</h3>\n<p>The above diagram depicts the high-level design of I/O threading in Valkey 8.\nI/O threads are worker threads that receive jobs to execute from the main thread. \nA job can involve reading and parsing a command from a client, writing responses back to the client, polling for I/O events on TCP connections, or deallocating memory.\nWhile I/O threads are busy handling I/O, the main thread is able to spend more time executing commands. </p>\n<p>The main thread orchestrates all the jobs spawned to the I/O threads, ensuring that no race conditions occur. \nThe number of active I/O threads can be adjusted by the main thread based on the current load to ensure efficient utilization of the underlying hardware. \nDespite the dynamic nature of I/O threads, the main thread maintains thread affinity, ensuring that, when possible the same I/O thread will handle I/O for the same client to improve memory access locality. </p>\n<p>Socket polling system calls, such as <code>epoll_wait</code>, are expensive procedures. \nWhen executed solely by the main thread, <code>epoll_wait</code> consumes more than 20 percent of the time. \nTherefore, we decided to offload <code>epoll_wait</code> execution to the I/O threads in the following way: to avoid race conditions, at any given time, at most one thread, either an io_thread or the main thread, executes <code>epoll_wait</code>. \nI/O threads never sleep on <code>epoll</code>, and whenever there are pending I/O operations or commands to be executed, <code>epoll_wait</code> calls are scheduled to the I/O threads by the main thread. \nIn all other cases, the main thread executes the <code>epoll_wait</code> with the waiting time as in the original Valkey implementation</p>\n<p>In addition, before executing commands, the main thread performs a new procedure, prefetch-commands-keys, which aims to reduce the number of external memory accesses needed when executing the commands on the main dictionary. A detailed explanation of the technique used in that procedure will be described in our next blog</p>\n<h3>Testing and Availability</h3>\n<p>The enhanced performance will be available for testing in the first release candidate of Valkey, available today.</p>\n",
    "slug": "unlock-one-million-rps",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "dantouitou",
      "uriyagelnik"
    ],
    "trending": false
  },
  {
    "title": "Valkey 8.0: Delivering Enhanced Performance and Reliability",
    "date": "2024-08-02T07:01:01.000Z",
    "excerpt": "The first release candidate of Valkey 8.0 is now available! Come learn about the exciting improvements in performance, reliability, and observability that are available in this new version.",
    "content": "<p>The Valkey community is proud to unveil the first release candidate of Valkey 8.0,\na major update designed to enhance performance, reliability, and observability \nfor all Valkey installations. In this blog, we&#39;ll dive a bit deeper into each of these \nareas and talk about the exciting features we&#39;ve built for this release.</p>\n<h2>Performance</h2>\n<p>Valkey 8.0 features significant improvements to the existing I/O threading system,\nallowing the main thread and I/O threads to operate concurrently. This release also\nincludes a number of improvements to offload work to the I/O threads and introduces\nefficient batching of commands. Altogether, Valkey 8.0 is designed to handle up to\n1.2 million Queries Per Second (QPS) on AWS&#39;s r7g platform, compared to the previous\nlimit of 380K QPS. We&#39;ll dive deeper into these numbers in an upcoming blog.</p>\n<p>NOTE: Not all improvements are available in the release candidate, but they will\nbe available in the GA release of Valkey 8.0.</p>\n<ul>\n<li><strong>Asynchronous I/O Threading</strong>: Enables parallel processing of commands and\nI/O operations, maximizing throughput and minimizing bottlenecks.</li>\n<li><strong>Intelligent Core Utilization</strong>: Distributes I/O tasks across multiple\ncores based on realtime usage, reducing idle time and improving energy efficiency.</li>\n<li><strong>Command Batching</strong>: Optimizes memory access patterns by prefetching frequently\naccessed data to minimize CPU cache misses, reducing memory accesses required for\ndictionary operations.</li>\n</ul>\n<p>For more details on these improvements, you can refer to\n<a href=\"https://github.com/valkey-io/valkey/pull/758\">#758</a> and\n<a href=\"https://github.com/valkey-io/valkey/pull/763\">#763</a>.</p>\n<h2>Reliability</h2>\n<p>Cluster scaling operations via slot migrations have historically been delicate.\nValkey 8.0 improves reliability and minimizes disruptions with the following\nenhancements:</p>\n<ul>\n<li><strong>Automatic Failover for Empty Shards</strong>: New shards that start empty, owning\nno slots, now benefit from automatic failover. This ensures high availability\nfrom the start of the scaling process.</li>\n<li><strong>Replication of Slot Migration States</strong>: All <code>CLUSTER SETSLOT</code> commands are\nnow replicated synchronously to replicas before execution on the primary. This\nreduces the chance of unavailability if the primary fails, as the replicas have\nthe most up-to-date information about the state of the shard. New replicas also\nautomatically inherit the state from the primary without additional input from\nan operator.</li>\n<li><strong>Slot Migration State Recovery</strong>: In the event of a failover, Valkey 8.0 automatically\nupdates the slot migration states on source and target nodes. This ensures requests\nare continuously routed to the correct primary in the target shard, maintaining\ncluster integrity and availability.</li>\n</ul>\n<p>For more details on these improvements, you can refer to\n<a href=\"https://github.com/valkey-io/valkey/pull/445\">#445</a>.</p>\n<h2>Replication</h2>\n<p>Valkey 8.0 introduces a dual-channel replication scheme, allowing the RDB and\nthe replica backlog to be transferred simultaneously, accelerating synchronization.</p>\n<ul>\n<li><strong>Reduced Memory Load</strong>: By streaming replication data to the replica during\nthe full sync, the primary node experiences significantly less memory pressure.\nThe replica now manages the Client Output Buffer (COB) tracking, reducing the\nlikelihood of COB overruns and enabling larger COB sizes on the replica side.</li>\n<li><strong>Reduced Parent Process Load</strong>: A dedicated connection for RDB transfer frees\nthe primary&#39;s parent process from handling this data, allowing it to focus on\nclient queries and improving overall responsiveness.</li>\n</ul>\n<p>Performance tests show improvements in write latency during sync, and in scenarios\nwith heavy read commands, the sync time can be cut by up to 50%. This translates\nto a more responsive system, even during synchronization.</p>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/60\">#60</a>.</p>\n<h2>Observability</h2>\n<p>Valkey 8.0 introduces a comprehensive per-slot metrics infrastructure, providing\ndetailed visibility into the performance and resource usage of individual slots.\nThis granular data helps inform decisions about resource allocation, load\nbalancing, and performance optimization.</p>\n<ul>\n<li><strong>Key Count</strong>: Returns the number of keys in each slot, making it easier to\nidentify the slots with the largest number of keys.</li>\n<li><strong>CPU Usage</strong>: Tracks CPU time consumed by operations on each slot, identifying\nareas of high utilization and potential bottlenecks.</li>\n<li><strong>Network Input/Output Bytes</strong>: Monitors data transmission and reception by\neach slot, offering insights into network load and bandwidth utilization.</li>\n<li><strong>Minimal Overhead</strong>: Initial benchmarks show that enabling detailed metrics\nincurs a negligible overhead of approximately 0.7% in QPS.</li>\n</ul>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/712\">#712</a>,\n<a href=\"https://github.com/valkey-io/valkey/pull/720\">#720</a>, and <a href=\"https://github.com/valkey-io/valkey/pull/771\">#771</a>.</p>\n<h2>Efficiency</h2>\n<p>Valkey 8.0 introduces two new improvements that reduce the memory overhead of keys,\nallowing users to store more data without any application changes.\nThe first change is that keys are now embedded in the main dictionary, eliminating separate\nkey pointers and significantly reducing memory overhead. This results in a 9-10%\nreduction in overall memory usage for scenarios with 16-byte keys and 8 or 16-byte\nvalues, along with performance improvements.</p>\n<p>This release also introduces a new per-slot dictionary for Valkey cluster, which\nreplaces a linked list that used to allow operator to list out all the keys in\na slot for slot-migration. The new architecture splits the main dictionary by slot,\nreducing the memory overhead by 16 bytes per key-value pair without degrading performance. </p>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/541\">#541</a>\nand <a href=\"https://github.com/redis/redis/pull/11695\">Redis#11695</a>.</p>\n<h2>Additional Highlights</h2>\n<ul>\n<li><strong>Dual IPv4 and IPv6 Stack Support</strong>: Seamlessly operate in mixed IP environments\nfor enhanced compatibility and flexibility.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/736\">#736</a> for details.</li>\n<li><strong>Improved Pub/Sub Efficiency</strong>: Lightweight cluster messages streamline\ncommunication and reduce overhead for faster, more efficient Pub/Sub operations.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/654\">#654</a> for details.</li>\n<li><strong>Valkey Over RDMA (Experimental)</strong>: Unlock significant performance improvements\nwith direct memory access between clients and Valkey servers, delivering up to\n275% increase in throughput.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/477\">#477</a> for details.</li>\n<li><strong>Numerous Smaller Performance/Reliability Enhancements</strong>: Many under-the-hood\nimprovements ensure a smoother, more stable experience across the board.\nSee <a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc1\">release notes</a> for details.</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Valkey 8.0 is a major update that offers improved performance, reliability, and\nobservability. Whether you are an experienced Valkey/Redis user or exploring\nit for the first time, this release provides significant advancements in in-memory\ndata storage. You can try out these enhancements today by downloading from \n<a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc1\">source</a> or using one \nof our <a href=\"https://hub.docker.com/r/valkey/valkey\">container images</a>. We would love \nto hear your thoughts on these new features and what you hope to see in the \nfuture from the Valkey project.</p>\n<p><strong>Important Note</strong>: The Valkey Over RDMA feature is currently experimental and\nmight change or be removed in future versions.</p>\n<p>We look forward to seeing what you achieve with Valkey 8.0!</p>\n",
    "slug": "valkey-8-0-0-rc1",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "pingxie",
      "madolson"
    ],
    "trending": false
  },
  {
    "title": "What's new in Valkey for June 2024",
    "date": "2024-06-27T07:01:01.000Z",
    "excerpt": "What have people been saying about Valkey in June 2024? Read on to find out.",
    "content": "<p>What have people been saying since the <a href=\"/blog/may-roundup/\">last what&#39;s new post</a>? Read on to find out.</p>\n<h2>News and Press</h2>\n<p>Richard Speed from <a href=\"https://www.theregister.com/2024/06/19/valkey_picks_up_more_partners/\">The Register tells about how Valkey gained momentum with new backers</a>.\nMichael Larabel wrote a couple of stories on Phoronix: one on how <a href=\"https://www.phoronix.com/news/Fedora-Replacing-Redis-Valkey\">Valkey will be packaged in Fedora 41</a> and another on <a href=\"https://www.phoronix.com/news/Valkey-Redis-Fork-More-Backers\">Valkey&#39;s new backers</a>.</p>\n<h2>Valkey How-To</h2>\n<p>Abhishek Gupta gives a <a href=\"https://community.aws/content/2hx81ITCvDiWqrAz06SECOvepoa/getting-started-with-valkey-using-javascript\">tutorial on how to get started with Valkey on JavaScript (and LangChain)</a>.</p>\n<p>In less than 10 minutes Shantanu shows you how to install Valkey from source on Ubuntu.</p>\n<p>{{ youtube(id=&quot;T-tH1GC0omo&quot;) }}</p>\n<p>Percona published a couple of detailed how-to posts: Matthew Boehm detailed <a href=\"https://www.percona.com/blog/valkey-redis-setting-up-replication/\">setting up Valkey replication</a> and Anil Joshi <a href=\"https://www.percona.com/blog/valkey-redis-sharding-using-the-native-clustering-feature/\">covers Valkey sharding</a>.</p>\n<h2>Interviews and Podcasts</h2>\n<p>Swapnil from <a href=\"https://tfir.io/why-open-source-still-leads-the-way-despite-license-changes-ann-schlemmer/\">TFIR interviews Ann Schlemmer from Percona</a> about why open source still leads the way despite license changes.</p>\n<p>{{ youtube(id=&quot;D2G7kfAO37U&quot;) }}</p>\n<p>TSC member <a href=\"https://www.odbms.org/2024/06/on-the-open-source-valkey-project-qa-with-madelyn-olson/\">Madelyn Olson was interviewed by Roberto Zicari on ODBMS</a> and chatted with <a href=\"https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/steering-through-open-source-waters-with-madelyn-olson/\">Corey Quinn about Valkey on the Screaming in the Cloud podcast</a>.</p>\n<p>{{ youtube(id=&quot;Pl-udfEPwtk&quot;) }}</p>\n<p>Robert and Courtney from the WooCommerce community chat about Valkey on the <a href=\"https://www.youtube.com/watch?v=E1hX1GZij_U\">Do the Woo Podcast</a>.</p>\n<p>{{ youtube(id=&quot;E1hX1GZij_U&quot;) }}</p>\n<h2>Want to feature your tutorial/article/meetup/video?</h2>\n<p>Add your own links to the <a href=\"https://github.com/valkey-io/valkey-io.github.io/pulls?q=is%3Apr+is%3Aopen+label%3Aroundup-post\">draft pull request open on the website GitHub repo</a>.\nYou can also submit your own content to be published directly on valkey.io by following the <a href=\"https://github.com/valkey-io/valkey-io.github.io/blob/main/CONTRIBUTING-BLOG-POST.md\">blog post contributing guide</a>.</p>\n",
    "slug": "whats-new-june-2024",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "What's new in Valkey for May 2024",
    "date": "2024-05-24T07:01:01.000Z",
    "excerpt": "It's become clear that people want to talk about Valkey and have been publishing blog posts/articles fervently. Here you'll find a collection of all the post I'm aware of in the last few weeks.",
    "content": "<p>It&#39;s become clear that people want to talk about Valkey and have been publishing blog posts/articles fervently.\nHere you&#39;ll find a collection of all the post I&#39;m aware of in the last few weeks.</p>\n<h2>Percona</h2>\n<p>The kind folks over at Percona have been on an absolutely legendary streak of posting about Valkey.\nThey&#39;ve done a series on data types (<a href=\"https://www.percona.com/blog/valkey-redis-the-hash-datatype/\">Hashes</a>, <a href=\"https://www.percona.com/blog/valkey-redis-sets-and-sorted-sets/\">Sorted Sets</a>), <a href=\"https://www.percona.com/blog/valkey-redis-configuration-best-practices/\">best</a> and <a href=\"https://www.percona.com/blog/valkey-redis-not-so-good-practices/\">not-so-good practices</a>, <a href=\"https://www.percona.com/blog/hello-valkey-lets-get-started/\">getting started</a>, <a href=\"https://www.percona.com/blog/valkey-redis-replication-and-auto-failover-with-sentinel-service/\">replication/failover</a>, <a href=\"https://www.percona.com/blog/valkey-redis-configurations-and-persistent-setting-of-the-key-parameters/\">configurations/persistence</a>, and finally their own <a href=\"https://www.percona.com/blog/hello-valkey-lets-get-started/\">Valkey packages for DEB and RPM-based distros</a>.</p>\n<h2>Fedora Magazine</h2>\n<p>Yours truly wrote an article for <a href=\"https://fedoramagazine.org/how-to-move-from-redis-to-valkey/\">Fedora Magazine about using the <code>valkey-compat-redis</code> package to move to Valkey</a>.</p>\n<h2>Community.aws</h2>\n<p>Ricardo Ferreira put together a <a href=\"https://community.aws/content/2fdr6Vg8BiJS8jr8xsuQRRc0MD5/getting-started-with-valkey-using-docker-and-go\">walkthrough of using Valkey with Go on Docker</a>.</p>\n<h2>The New Stack</h2>\n<p>While Open Source Summit North America was last month, <a href=\"https://thenewstack.io/valkey-a-redis-fork-with-a-future/\">The New Stack published a blog post about Valkey</a> and accompany interview with project leaders, it&#39;s worth a watch and read.</p>\n<h2>Presentation: Digging into Valkey</h2>\n<p>On the subject of Open Source Summit, the talk I gave along side Madelyn Olson, <a href=\"https://youtu.be/3G6QgwIl-xs\">&quot;Digging into Valkey&quot; was posted as a video</a>.</p>\n<h2>Valkey Seattle IRL</h2>\n<p>The <a href=\"https://www.meetup.com/seattle-valkey/\">Seattle Valkey Meetup</a> is holding a <a href=\"https://www.meetup.com/seattle-valkey/events/301177195/\">Rust module workshop on June 5th</a>.\nA lot of folks will be in town for the Contributor Summit, so this meet up is bound to  be flush with Valkey experts.\nDon&#39;t miss it.</p>\n<h2>Want to add your tutorial/article/meetup/video to a future roundup?</h2>\n<p>This is the first in a series of roundups on Valkey content.\nThe plan is to keep an <a href=\"https://github.com/valkey-io/valkey-io.github.io/issues?q=is%3Adraft+label%3Aroundup-post+\">draft pull request open on the website GitHub repo</a> where you can contribute your own content.</p>\n",
    "slug": "may-roundup",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Valkey Modules 101",
    "date": "2024-05-01T07:01:01.000Z",
    "excerpt": "The idea of modules is to allow adding extra features (such as new commands and data types) to Valkey without making changes to the core code.",
    "content": "<h2>What are Valkey modules?</h2>\n<p>The idea of modules is to allow adding extra features (such as new commands and data types) to Valkey without making changes to the core code.\nModules are a special type of code distribution called a shared library, which can be loaded by other programs at runtime and executed.\nModules can be written in C or other languages that have C bindings.\nIn this article we will go through the process of building simple modules in C and Rust (using the Valkey Module Rust SDK).\nThis article expects the audience to be at least somewhat familiar with git, C, Rust and Valkey.</p>\n<h2>Hello World module in C</h2>\n<p>If we clone the Valkey repo by running <code>git clone git@github.com:valkey-io/valkey.git</code> we will find numerous examples in <code>src/modules</code>.\nLet&#39;s create a new file <code>module1.c</code> in the same folder.</p>\n<pre><code class=\"language-c\">#include &quot;../valkeymodule.h&quot;\n\nint hello(ValkeyModuleCtx *ctx, ValkeyModuleString **argv, int argc) {\n    VALKEYMODULE_NOT_USED(argv);\n    VALKEYMODULE_NOT_USED(argc);\n    return ValkeyModule_ReplyWithSimpleString(ctx, &quot;world1&quot;);\n}\n\nint ValkeyModule_OnLoad(ValkeyModuleCtx *ctx, ValkeyModuleString **argv, int argc) {\n    VALKEYMODULE_NOT_USED(argv);\n    VALKEYMODULE_NOT_USED(argc);\n    if (ValkeyModule_Init(ctx,&quot;module1&quot;,1,VALKEYMODULE_APIVER_1) \n        == VALKEYMODULE_ERR) return VALKEYMODULE_ERR;\n    if (ValkeyModule_CreateCommand(ctx,&quot;module1.hello&quot;, hello,&quot;&quot;,0,0,0) \n        == VALKEYMODULE_ERR) return VALKEYMODULE_ERR;\n    return VALKEYMODULE_OK;\n}\n</code></pre>\n<p>Here we are calling <code>ValkeyModule_OnLoad</code> C function (required by Valkey) to initialize <code>module1</code> using <code>ValkeyModule_Init</code>.\nThen we use <code>ValkeyModule_CreateCommand</code> to create a Valkey command <code>hello</code> which uses C function <code>hello</code> and returns <code>world1</code> string.\nIn future blog posts we will expore these areas at greater depth.</p>\n<p>Now we need to update <code>src/modules/Makefile</code></p>\n<pre><code class=\"language-makefile\">all: ... module1.so\n\nmodule1.xo: ../valkeymodule.h\n\nmodule1.so: module1.xo\n    $(LD) -o $@ $^ $(SHOBJ_LDFLAGS) $(LIBS) -lc\n</code></pre>\n<p>Run <code>make module1.so</code> inside <code>src/modules</code> folder.\nThis will compile our module in the <code>src/modules</code> folder.</p>\n<h2>Hello World module in Rust</h2>\n<p>We will create a new Rust package by running <code>cargo new --lib module2</code> in bash.\nInside the <code>module2</code> folder we will have <code>Cargo.toml</code> and <code>src/lib.rs</code> files.\nTo install the valkey-module SDK run <code>cargo add valkey-module</code> inside <code>module2</code> folder.\nAlternativley we can add <code>valkey-module = &quot;0.1.0</code> in <code>Cargo.toml</code> under <code>[dependencies]</code>.\nRun <code>cargo build</code> and it will create or update the <code>Cargo.lock</code> file.</p>\n<p>Modify <code>Cargo.toml</code> to specify the crate-type to be &quot;cdylib&quot;, which will tell cargo to build the target as a shared library.\nRead <a href=\"https://doc.rust-lang.org/reference/linkage.html\">Rust docs</a> to understand more about <code>crate-type</code>.</p>\n<pre><code>[lib]\ncrate-type = [&quot;cdylib&quot;]\n</code></pre>\n<p>Now in <code>src/lib.rs</code> replace the existing code with the following:</p>\n<pre><code class=\"language-rust\">#[macro_use]\nextern crate valkey_module;\n\nuse valkey_module::{Context, ValkeyResult, ValkeyString, ValkeyValue};\n\nfn hello(_ctx: &amp;Context, _args: Vec&lt;ValkeyString&gt;) -&gt; ValkeyResult {\n    Ok(ValkeyValue::SimpleStringStatic(&quot;world2&quot;))\n}\n\nvalkey_module! {\n    name: &quot;module2&quot;,\n    version: 1,\n    allocator: (valkey_module::alloc::ValkeyAlloc, valkey_module::alloc::ValkeyAlloc),\n    data_types: [],\n    commands: [\n        [&quot;module2.hello&quot;, hello, &quot;&quot;, 0, 0, 0],\n    ]\n}\n</code></pre>\n<p>Rust syntax is a bit different than C but we are creating <code>module2</code> with command <code>hello</code> that returns <code>world2</code> string.\nWe are using the external crate <code>valkey_module</code> with <a href=\"https://doc.rust-lang.org/book/ch19-06-macros.html\">Rust macros</a> and passing it variables like <code>name</code> and <code>version</code>.\nSome variables like <code>data_types</code> and <code>commands</code> are arrays and we can pass zero, one or many values.\nSince we are not using ctx or args we prefix them with <code>_</code> (Rust convention) instead of <code>VALKEYMODULE_NOT_USED</code> as we did in C.</p>\n<p>Run <code>cargo build</code> in the root folder.\nWe will now see <code>target/debug/libmodule2.dylib</code> (on macOS).\nThe build will produce *.so files on Linux and *.dll files on Windows.</p>\n<h2>Run Valkey server with both modules</h2>\n<p>Go back into the Valkey repo folder and run <code>make</code> to compile the Valkey code.\nThen add these lines to the bottom of the <code>valkey.conf</code> file.</p>\n<pre><code>loadmodule UPDATE_PATH_TO_VALKEY/src/modules/module1.so\nloadmodule UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib\n</code></pre>\n<p>and run <code>src/valkey-server valkey.conf</code>.\nYou will see these messages in the log output.</p>\n<pre><code>Module &#39;module1&#39; loaded from UPDATE_PATH_TO_VALKEY/src/modules/module1.so\n...\nModule &#39;module2&#39; loaded from UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib\n</code></pre>\n<p>Then use <code>src/valkey-cli</code> to connect.</p>\n<pre><code class=\"language-bash\">src/valkey-cli -3\n127.0.0.1:6379&gt; module list\n1) 1# &quot;name&quot; =&gt; &quot;module2&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n2) 1# &quot;name&quot; =&gt; &quot;module1&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_VALKEY/src/modules/module1.so&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n127.0.0.1:6379&gt; module1.hello\nworld1\n127.0.0.1:6379&gt; module2.hello\nworld2\n</code></pre>\n<p>We can now run both modules side by side and if we modify either C or RS file, recompile the code and restart <code>valkey-server</code> we will get the new functionality.</p>\n<p>As an alternative to specifying modules in <code>valkey.conf</code> file, we can use <code>MODULE LOAD</code> and <code>UNLOAD</code> from <code>valkey-cli</code> to update the server.\nFirst specify <code>enable-module-command yes</code> in <code>valkey.conf</code> and restart <code>valkey-server</code>.\nThis enables us to update our module code, recompile it and reload it at runtime.</p>\n<pre><code>127.0.0.1:6379&gt; module load UPDATE_PATH_TO_VALKEY/src/modules/module1.so\nOK\n127.0.0.1:6379&gt; module list\n1) 1# &quot;name&quot; =&gt; &quot;module1&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_VALKEY/src/modules/module1.so&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n127.0.0.1:6379&gt; module unload module1\nOK\n127.0.0.1:6379&gt; module list\n(empty array)\n127.0.0.1:6379&gt; \n</code></pre>\n<p>Please stay tuned for more articles in the future as we explore the possibilities of Valkey modules and where using C or Rust makes sense.</p>\n<h2>Usefull links</h2>\n<ul>\n<li><a href=\"https://github.com/valkey-io/valkey\">Valkey repo</a></li>\n<li><a href=\"https://github.com/valkey-io/valkeymodule-rs\">Valkey Rust SDK</a></li>\n<li><a href=\"https://code.visualstudio.com/docs/languages/rust\">Rust in VS Code</a></li>\n</ul>\n",
    "slug": "modules-101",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "dmitrypol"
    ],
    "trending": false
  },
  {
    "title": "Valkey 7.2.5 GA is out!",
    "date": "2024-04-16T07:01:01.000Z",
    "excerpt": "Exciting times!I'm pleased to announce that you can start using the first generally available, stable Valkey release today.",
    "content": "<p>Exciting times!</p>\n<p>I&#39;m pleased to announce that you can start using the first generally available, stable Valkey release today.\nCheck out the <a href=\"/download/releases/v7-2-5\">release page for 7.2.5</a>.</p>\n<p>This release maintains the same protocol, API, return values, and data file formats with the last open source release of Redis (7.2.4).</p>\n<p>You can <a href=\"https://github.com/valkey-io/valkey/releases/tag/7.2.5\">build it from source</a> or <a href=\"https://hub.docker.com/r/valkey/valkey/\">pull it from Valkey’s official Docker Hub</a>.\nValkey’s release candidates are available in Fedora and EPEL and the new release will be available once the community updates the packages.</p>\n",
    "slug": "valkey-7-2-5-out",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "SET first-blog-post \\\"Hello, world\\\"",
    "date": "2024-04-11T07:01:01.000Z",
    "excerpt": "Welcome! For the inaugural blog post on valkey.io, I’d like to recap the story so far, what to look forward to, and then describe how this blog works.",
    "content": "<p>Welcome!\nFor the inaugural blog post on valkey.io, I’d like to recap the story so far, what to look forward to, and then describe how this blog works.</p>\n<h2>How do you describe an open source whirlwind?</h2>\n<p>I would describe it like this: first <a href=\"https://github.com/redis/redis/pull/13157\">a license change</a>, the <a href=\"https://github.com/valkey-io/valkey/commit/38632278fd06fe186f7707e4fa099f666d805547#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5\">establishment of PlaceholderKV</a>, a new name and <a href=\"https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community\">formation of Valkey within the Linux Foundation</a>, <a href=\"https://github.com/valkey-io/valkey/compare/redis-7.2.4...7.2.4-rc1\">hundreds of code updates</a> by community members from around the world, and <a href=\"https://github.com/valkey-io/valkey/releases/tag/7.2.4-rc1\">a release candidate</a>.\nAll within the span of about three weeks.</p>\n<h2>Only the start</h2>\n<p>Out of this initial flurry of activity emerges a project that is sure to have a long history.\nThis blog will cover the project over time by describing what’s new, what to look forward to, and how you can explore the full extent of Valkey.</p>\n<h2>For the community, with the community</h2>\n<p>Like the Valkey project itself, this blog is not a singular effort of one company but rather a community effort, <a href=\"https://github.com/valkey-io/valkey-io.github.io/\">built in the open with full transparency</a>.\nYou want to write about a topic on the blog?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/fork\">Fork it and make a pull request.</a>\nYou want to help edit or review a post?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues\">Do a code review.</a>\nProblem with a post?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues/new?assignees=&labels=bug%2C+untriaged&projects=&template=bug_template.md&title=%5BBUG%5D\">Create an issue.</a>\nFeel like something is missing?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues/new?assignees=&labels=enhancement&projects=&template=feature_template.md&title=\">Make a feature request.</a></p>\n<h2>What’s next?</h2>\n<p>Stay tuned for trip reports from Valkey’s first conferences then information about the first GA release.\nIt’s only going to get more exciting from here.</p>\n",
    "slug": "hello-world",
    "category": "news",
    "imageUrl": "/src/assets/media/blog/default.png",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Getting Started with Valkey",
    "date": "2024-03-23T00:00:00.000Z",
    "excerpt": "Learn how to get started with Valkey in this comprehensive tutorial that covers installation, basic usage, and best practices.",
    "content": "<h1>Getting Started with Valkey</h1>\n<p>Welcome to this comprehensive guide on getting started with Valkey. In this tutorial, we&#39;ll walk through everything you need to know to begin using Valkey effectively.</p>\n<h2>Installation</h2>\n<p>First, let&#39;s install Valkey. You can do this using npm:</p>\n<pre><code class=\"language-bash\">npm install valkey\n</code></pre>\n<p>Or if you prefer using yarn:</p>\n<pre><code class=\"language-bash\">yarn add valkey\n</code></pre>\n<h2>Basic Usage</h2>\n<p>Once installed, you can start using Valkey in your project:</p>\n<pre><code class=\"language-typescript\">import { Valkey } from &#39;valkey&#39;;\n\nconst valkey = new Valkey({\n  // your configuration options here\n});\n</code></pre>\n<h2>Next Steps</h2>\n<p>Stay tuned for more tutorials and guides on using Valkey effectively in your projects. We&#39;ll cover:</p>\n<ul>\n<li>Advanced configuration options</li>\n<li>Best practices</li>\n<li>Common use cases</li>\n<li>Troubleshooting tips</li>\n</ul>\n<p>Feel free to reach out to our community if you have any questions! </p>\n",
    "slug": "getting-started-with-valkey",
    "category": "tutorials",
    "imageUrl": "/images/blog/getting-started.jpg",
    "authorUsernames": [
      "stockholmux"
    ],
    "trending": true
  }
];

export const blogPosts: BlogPost[] = blogPostsRaw.map(post => ({
  ...post,
  authors: post.authorUsernames.map(username => 
    authors.find(a => a.username === username) || {
      name: 'Unknown Author',
      username,
      bio: 'No bio available',
      imageUrl: '/images/authors/default.jpg',
      role: 'Contributor'
    }
  )
}));

export const blogDigest = blogPosts.map(({ title, date, excerpt, slug, category, imageUrl, authors, trending }) => ({
  title,
  date,
  excerpt,
  slug,
  category,
  imageUrl,
  authors,
  trending
}));
